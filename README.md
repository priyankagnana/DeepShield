# ğŸ›¡ï¸ DeepShield â€” Real-Time Deepfake Detection System

A fully offline, explainable deepfake detection system built on **EfficientNet-B0** with Grad-CAM visual explanations, a polished Streamlit UI, and a FastAPI backend for real-time inference.

---

## ğŸ“Œ Overview

Deepfake technology uses generative AI to create highly realistic synthetic faces in images and videos. While powerful, it poses serious risks â€” misinformation, identity fraud, impersonation, and reputational harm.

**DeepShield** addresses this with a privacy-first, fully offline detection pipeline that:

- Classifies images and videos as âœ… **Real** or ğŸš¨ **Fake**
- Provides **confidence scores** and **P(Real) / P(Fake)** probabilities
- Explains decisions visually using **Grad-CAM heatmaps**
- Runs entirely on your machine â€” **no cloud calls, no data leaves your device**

---
## Drive link 
https://drive.google.com/drive/folders/1kyWpFCtVWmF7qZGzEp4oOrDCnI8K6OAN?role=writer

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input (Image /    â”‚
â”‚   Video / Webcam)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Face Detection    â”‚  â† OpenCV Haar Cascade (face_detector.py)
â”‚   & Frame Sampling  â”‚  â† Frame extractor (frame_extractor.py)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image Preprocessingâ”‚  â† Resize 224Ã—224, ImageNet normalize
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DeepfakeCNN Model            â”‚
â”‚                                      â”‚
â”‚  EfficientNet-B0 (Spatial Branch)    â”‚  â†’ 1280-dim features
â”‚  +                                   â”‚
â”‚  FrequencyBranch (FFT Spectrum)      â”‚  â†’ 128-dim features  [opt-in]
â”‚                                      â”‚
â”‚  Fused â†’ Linear head â†’ Binary logit  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Classification     â”‚  Real / Fake + confidence score
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grad-CAM Module    â”‚  Visual heatmap over suspicious regions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

| Feature | Details |
|---|---|
| **EfficientNet-B0 backbone** | ImageNet-pretrained, two-phase fine-tuning |
| **Frequency-domain analysis** | Optional FFT branch detects GAN grid artefacts |
| **Face detection** | OpenCV Haar cascade â€” crops to face before inference |
| **Grad-CAM explanations** | Heatmap overlay showing which regions drove the decision |
| **Full video analysis** | Samples N frames evenly, aggregates with majority vote + timeline chart |
| **Live webcam** | `streamlit-webrtc` in the UI + CLI realtime script |
| **FastAPI backend** | REST + WebSocket endpoints for image, video, and frame streaming |
| **Fully offline** | No internet connection required for inference |
| **MPS / CUDA / CPU** | Auto-detects Apple Silicon, NVIDIA GPU, or CPU |

---

## ğŸ“‚ Project Structure

```
DeepShield/
â”‚
â”œâ”€â”€ api/                        â† FastAPI backend
â”‚   â”œâ”€â”€ main.py                 â† App entry point, model loaded at startup
â”‚   â”œâ”€â”€ schemas.py              â† Pydantic response models
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ predict.py          â† POST /predict/image, POST /predict/video
â”‚       â””â”€â”€ stream.py           â† WS /ws/webcam (real-time frame inference)
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ cnn_model.py            â† DeepfakeCNN (EfficientNet-B0 + optional FrequencyBranch)
â”‚   â”œâ”€â”€ frequency_branch.py     â† FFT-based spectral feature extractor
â”‚   â””â”€â”€ loss.py
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ predict.py              â† load_model, predict, predict_image, predict_video, predict_with_gradcam
â”‚   â””â”€â”€ realtime_inference.py   â† CLI webcam / video loop with frame skipping
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py                â† Two-phase EfficientNet fine-tuning
â”‚   â”œâ”€â”€ evaluate.py             â† Test-set evaluation with tqdm progress
â”‚   â”œâ”€â”€ dataset.py              â† DataLoader, balanced subset sampling
â”‚   â”œâ”€â”€ metrics.py              â† Accuracy, precision, recall, F1, confusion matrix
â”‚   â””â”€â”€ early_stopping.py
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ face_detector.py        â† detect_and_crop_face() using OpenCV Haar cascade
â”‚   â”œâ”€â”€ frame_extractor.py      â† Extract 30 frames/video with multiprocessing
â”‚   â”œâ”€â”€ dataset_split.py        â† Sort raw videos â†’ real/ fake/ using metadata.json
â”‚   â”œâ”€â”€ split_train_val_test.py â† 70/15/15 split grouped by video ID
â”‚   â””â”€â”€ augmentations.py
â”‚
â”œâ”€â”€ explainability/
â”‚   â”œâ”€â”€ gradcam.py              â† Grad-CAM with forward + backward hooks
â”‚   â””â”€â”€ heatmap_utils.py        â† Heatmap colormap overlay
â”‚
â”œâ”€â”€ saved_models/
â”‚   â””â”€â”€ best_model.pth          â† Best checkpoint saved during training
â”‚
â”œâ”€â”€ app.py                      â† Streamlit UI (Image / Video / Webcam tabs)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Tech Stack

| Category | Tools |
|---|---|
| **Deep Learning** | PyTorch, TorchVision |
| **Model** | EfficientNet-B0 (ImageNet pretrained) |
| **Computer Vision** | OpenCV |
| **Frequency Analysis** | PyTorch FFT (`torch.fft.fft2`, fftshift) |
| **Explainability** | Grad-CAM (backward hooks) |
| **Frontend** | Streamlit, streamlit-webrtc, Plotly |
| **Backend API** | FastAPI, Uvicorn, WebSockets |
| **Data / Metrics** | NumPy, Pandas, Scikit-learn |
| **Training utilities** | tqdm, early stopping |

---

## ğŸ“Š Model Performance

Trained on the **140k Real vs Fake Faces** dataset (Kaggle):

| Metric | Score |
|---|---|
| Accuracy | ~91â€“92% |
| Precision | â€” |
| Recall | â€” |
| F1 Score | â€” |

> Run `python -m training.evaluate` after training to get exact numbers on your test split.

---

## ğŸ“¥ Dataset Setup

This project uses the **140k Real vs Fake Faces** dataset from Kaggle.

**Download link:** [https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces](https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces)

After downloading, place it at the project root:

```
DeepShield/
â””â”€â”€ 140k-faces/
    â””â”€â”€ real_vs_fake/
        â””â”€â”€ real-vs-fake/
            â”œâ”€â”€ train/
            â”‚   â”œâ”€â”€ real/
            â”‚   â””â”€â”€ fake/
            â”œâ”€â”€ valid/
            â”‚   â”œâ”€â”€ real/
            â”‚   â””â”€â”€ fake/
            â””â”€â”€ test/
                â”œâ”€â”€ real/
                â””â”€â”€ fake/
```

> The `140k-faces/` folder is in `.gitignore` and must be placed manually on each machine.

---

## ğŸš€ Full Setup & Workflow

### Prerequisites

#### 1. System libraries (macOS â€” install before creating the venv)

```bash
brew install xz cmake libomp
```

#### 2. Python version (3.10+ recommended)

```bash
pyenv install 3.12.2
pyenv local 3.12.2
```

#### 3. Virtual environment

```bash
python3 -m venv venv
source venv/bin/activate       # macOS / Linux
# venv\Scripts\activate        # Windows
```

#### 4. Install dependencies

```bash
pip install -r requirements.txt
```

---

### Step 1 â€” Train the Model

```bash
python -m training.train
```

Trains DeepfakeCNN using two-phase EfficientNet fine-tuning for up to 50 epochs. Best checkpoint is saved to `saved_models/best_model.pth` whenever validation accuracy improves.

**Training phases:**
- **Phase 1** (epochs 1â€“5): Backbone frozen, only the classifier head trains at lr=1e-4
- **Phase 2** (epoch 6+): Last two EfficientNet blocks unfrozen, full model trains at lr=1e-5

Sample output:
```
Epoch 1/50 [Ph1]  train_loss=0.512  val_loss=0.431  val_acc=0.8120
...
Epoch 20/50 [Ph2]  train_loss=0.214  val_loss=0.198  val_acc=0.9167
```

---

### Step 2 â€” Evaluate on the Test Set

```bash
python -m training.evaluate
```

Loads `saved_models/best_model.pth` and reports Accuracy, Precision, Recall, F1, and Confusion Matrix on the held-out test set. Includes a tqdm progress bar.

Sample output:
```
Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [05:23<00:00]

Test set evaluation
----------------------------------------
Accuracy:  0.9167
Precision: 0.9210
Recall:    0.9140
F1:        0.9175
```

---

### Step 3 â€” Launch the Streamlit App

```bash
streamlit run app.py
```

Opens the full UI at `http://localhost:8501`. Three tabs:

#### ğŸ“· Image Tab
- Upload any face image (JPG/PNG)
- Shows verdict card with confidence %, P(Real), P(Fake)
- Enable **Grad-CAM** in sidebar to see which facial regions influenced the decision
- Plotly donut chart shows Real/Fake probability split

#### ğŸ¬ Video Tab
- Upload a video (MP4/AVI/MOV)
- Choose how many frames to analyze (4â€“32)
- Summary metrics: frames analyzed, avg P(Real), real/fake frame counts
- Interactive **P(Real) timeline chart** (per-frame line chart with 0.5 threshold)
- **Frame distribution histogram** showing score spread
- Collapsible per-frame detail table

#### ğŸ“¹ Webcam Tab
- Live webcam feed via `streamlit-webrtc`
- Inference every 3rd frame to keep stream smooth
- Bottom banner shows Real/Fake label + confidence
- Top bar shows P(Real) as a fill indicator
- Falls back gracefully if `streamlit-webrtc` is not installed

**Sidebar options:**
- Toggle Grad-CAM overlay
- Score interpretation table (what P(Real) ranges mean)

---

### Step 4 â€” Run the FastAPI Backend

```bash
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

Model is loaded **once at startup** and reused for all requests.

Interactive API docs: `http://localhost:8000/docs`

| Endpoint | Method | Description |
|---|---|---|
| `/health` | GET | Check if model is loaded and which device is in use |
| `/predict/image` | POST | Upload image â†’ `{label, confidence, prob_real}` |
| `/predict/video` | POST | Upload video â†’ aggregated + per-frame results |
| `/ws/webcam` | WebSocket | Send JPEG bytes â†’ receive JSON predictions in real-time |

Example request (image):
```bash
curl -X POST http://localhost:8000/predict/image \
  -F "file=@face.jpg"
```

Example response:
```json
{
  "label": "Fake",
  "confidence": 0.9312,
  "prob_real": 0.0688
}
```

---

### Step 5 â€” CLI Real-Time Inference (Webcam or Video)

```bash
# Webcam
python -m inference.realtime_inference

# Video file
python -m inference.realtime_inference --video path/to/video.mp4

# With Grad-CAM overlay
python -m inference.realtime_inference --video path/to/video.mp4 --gradcam
```

Press **Q** to quit. Inference runs every 3rd frame for smooth display.

---

### Quick Reference

```bash
# â”€â”€ Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m training.train

# â”€â”€ Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m training.evaluate

# â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
streamlit run app.py

# â”€â”€ FastAPI backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uvicorn api.main:app --reload --port 8000

# â”€â”€ CLI webcam / video â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m inference.realtime_inference [--video <path>] [--gradcam]
```

---

## ğŸ”¬ Model Architecture Details

### DeepfakeCNN

```
EfficientNet-B0 (pretrained on ImageNet)
  â””â”€â”€ features[0..8]  (MBConv blocks)
  â””â”€â”€ classifier
        â”œâ”€â”€ Dropout(0.4)
        â””â”€â”€ Linear(1280 â†’ 1)          # Default mode

Optional: use_frequency=True
  EfficientNet features (1280-dim)
  + FrequencyBranch (128-dim)
  â†’ Linear(1408 â†’ 256) â†’ ReLU â†’ Dropout(0.4) â†’ Linear(256 â†’ 1)
```

### FrequencyBranch

Detects spectral artefacts characteristic of GAN-generated images:

1. `torch.fft.fft2` â€” 2D Fast Fourier Transform
2. `fftshift` â€” centres low-frequency content for spatially-consistent conv filters
3. `log1p` â€” compresses extreme dynamic range of FFT magnitudes
4. Two Conv2D + BatchNorm + MaxPool blocks
5. Fully connected â†’ 128-dim feature vector

### Two-Phase Training

| Phase | Epochs | LR | Backbone |
|---|---|---|---|
| Phase 1 (warm-up) | 1â€“5 | 1e-4 | Fully frozen |
| Phase 2 (fine-tune) | 6+ | 1e-5 | Last 2 blocks unfrozen |

---

## ğŸ“Š Evaluation Metrics

| Metric | Description |
|---|---|
| Accuracy | Overall correct classifications |
| Precision | Of predicted fakes, how many were actually fake |
| Recall | Of actual fakes, how many were caught |
| F1 Score | Harmonic mean of precision and recall |
| Confusion Matrix | True/False Positive/Negative breakdown |

---

## ğŸŒ Applications

- Social media content verification
- News authenticity validation
- Digital identity protection
- Cybercrime and fraud detection
- Media forensics and journalism

---

## ğŸ”® Future Enhancements

- Temporal modeling with 3D CNN or Vision Transformer across video frames
- Audio-visual consistency check (voice + face sync)
- Browser extension for in-page detection
- Mobile deployment (CoreML / TFLite)
- Confidence calibration and uncertainty estimation

---

## ğŸ“œ License

This project is released under the MIT License.
